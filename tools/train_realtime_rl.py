"""
ì‹¤ì‹œê°„ ê°•í™”í•™ìŠµ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ê²Œì„ì„ í”Œë ˆì´í•˜ë©´ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ í•™ìŠµ

ì‚¬ìš©ë²•: py tools/train_realtime_rl.py --timesteps 50000
"""
import argparse
from pathlib import Path
import sys
import time

sys.path.insert(0, str(Path(__file__).parent.parent))

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback
from src.rl_env_realtime import RealtimeGameEnv
import torch
import keyboard


class RealtimeTrainingCallback(BaseCallback):
    """ì‹¤ì‹œê°„ í•™ìŠµ ì½œë°±"""
    
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.current_episode_reward = 0
        self.current_episode_length = 0
        
    def _on_step(self):
        """ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œ"""
        # ESCë¡œ ì¤‘ì§€
        if keyboard.is_pressed('esc'):
            print("\nâ¹ï¸  ESC ê°ì§€ - í•™ìŠµ ì¤‘ì§€")
            return False
        
        # í†µê³„ ìˆ˜ì§‘
        self.current_episode_reward += self.locals['rewards'][0]
        self.current_episode_length += 1
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ
        if self.locals['dones'][0]:
            self.episode_rewards.append(self.current_episode_reward)
            self.episode_lengths.append(self.current_episode_length)
            
            # ìµœê·¼ 10 ì—í”¼ì†Œë“œ í‰ê· 
            if len(self.episode_rewards) >= 10:
                avg_reward = sum(self.episode_rewards[-10:]) / 10
                avg_length = sum(self.episode_lengths[-10:]) / 10
                
                print(f"\nğŸ“Š ì—í”¼ì†Œë“œ {len(self.episode_rewards)}")
                print(f"   ë³´ìƒ: {self.current_episode_reward:.2f}")
                print(f"   ê¸¸ì´: {self.current_episode_length} ìŠ¤í…")
                print(f"   í‰ê·  (ìµœê·¼ 10): {avg_reward:.2f} ë³´ìƒ, {avg_length:.0f} ìŠ¤í…")
            
            self.current_episode_reward = 0
            self.current_episode_length = 0
        
        return True


def main():
    parser = argparse.ArgumentParser(description="ì‹¤ì‹œê°„ RL í•™ìŠµ")
    parser.add_argument("--game", default="ML", help="ê²Œì„ ì´ë¦„")
    parser.add_argument("--timesteps", type=int, default=50000, help="í•™ìŠµ íƒ€ì„ìŠ¤í…")
    parser.add_argument("--learning-rate", type=float, default=0.0003, help="í•™ìŠµë¥ ")
    parser.add_argument("--frame-width", type=int, default=84, help="í”„ë ˆì„ ë„ˆë¹„")
    parser.add_argument("--frame-height", type=int, default=84, help="í”„ë ˆì„ ë†’ì´")
    parser.add_argument("--frame-stack", type=int, default=4, help="í”„ë ˆì„ ìŠ¤íƒ")
    parser.add_argument("--load-model", type=str, help="ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (ê³„ì† í•™ìŠµ)")
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ® ì‹¤ì‹œê°„ ê°•í™”í•™ìŠµ")
    print("=" * 60)
    print(f"ê²Œì„: {args.game}")
    print(f"íƒ€ì„ìŠ¤í…: {args.timesteps:,}")
    print(f"í•™ìŠµë¥ : {args.learning_rate}")
    print(f"í”„ë ˆì„ í¬ê¸°: {args.frame_width}x{args.frame_height}")
    print("=" * 60)
    
    # ì¤€ë¹„ í™•ì¸
    print("\nâš ï¸  ì‹¤ì‹œê°„ í•™ìŠµ ì£¼ì˜ì‚¬í•­:")
    print("  1. ê²Œì„ì´ ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•©ë‹ˆë‹¤")
    print("  2. ìºë¦­í„°ê°€ ì•ˆì „í•œ ë§µì— ìˆì–´ì•¼ í•©ë‹ˆë‹¤")
    print("  3. ë§ˆìš°ìŠ¤/í‚¤ë³´ë“œë¥¼ ê±´ë“œë¦¬ì§€ ë§ˆì„¸ìš”")
    print("  4. ROI ì„¤ì •ì´ ë˜ì–´ìˆì–´ì•¼ í•©ë‹ˆë‹¤ (py tools/setup_roi.py)")
    print("  5. ESCë¡œ ì–¸ì œë“  ì¤‘ì§€ ê°€ëŠ¥")
    print()
    
    # ROI ì„¤ì • í™•ì¸
    roi_path = Path("configs/roi_settings.json")
    if not roi_path.exists():
        print("âŒ ROI ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤!")
        print("   ë¨¼ì € 'py tools/setup_roi.py' ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”")
        return
    
    input("ì¤€ë¹„ë˜ë©´ ì—”í„°ë¥¼ ëˆ„ë¥´ì„¸ìš”... ")
    
    print("\nâ° 5ì´ˆ í›„ í•™ìŠµ ì‹œì‘...")
    for i in range(5, 0, -1):
        print(f"   {i}ì´ˆ...")
        time.sleep(1)
    
    # í™˜ê²½ ìƒì„±
    print("\nğŸ“Š í™˜ê²½ ìƒì„± ì¤‘...")
    env = RealtimeGameEnv(
        game=args.game,
        frame_width=args.frame_width,
        frame_height=args.frame_height,
        frame_stack=args.frame_stack
    )
    
    print(f"âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ")
    print(f"   ê´€ì¸¡ ê³µê°„: {env.observation_space.shape}")
    print(f"   í–‰ë™ ê³µê°„: {env.action_space.n}ê°œ")
    
    # ëª¨ë¸ ìƒì„± ë˜ëŠ” ë¡œë“œ
    if args.load_model:
        print(f"\nğŸ“‚ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ: {args.load_model}")
        model = PPO.load(args.load_model, env=env)
        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ê³„ì† í•™ìŠµ)")
    else:
        print("\nğŸ¤– PPO ëª¨ë¸ ìƒì„± ì¤‘...")
        
        policy_kwargs = dict(
            features_extractor_kwargs=dict(features_dim=512),
            net_arch=[512, 512]
        )
        
        model = PPO(
            "CnnPolicy",
            env,
            learning_rate=args.learning_rate,
            n_steps=1024,  # ë” ìì£¼ ì—…ë°ì´íŠ¸ (2048â†’1024)
            batch_size=32,  # ë” ì‘ì€ ë°°ì¹˜ë¡œ ë¹ ë¥¸ í•™ìŠµ
            n_epochs=10,
            gamma=0.99,
            gae_lambda=0.95,
            clip_range=0.2,
            ent_coef=0.05,  # íƒí—˜ ëŒ€í­ ì¦ê°€ (0.01â†’0.05)
            vf_coef=0.5,
            max_grad_norm=0.5,
            policy_kwargs=policy_kwargs,
            verbose=1,
            tensorboard_log=f"logs/realtime/{args.game}"
        )
        print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
    
    # ì½œë°± ì„¤ì •
    checkpoint_dir = Path(f"models/realtime/{args.game}/checkpoints")
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=str(checkpoint_dir),
        name_prefix=f"{args.game}_ppo_realtime"
    )
    
    training_callback = RealtimeTrainingCallback(verbose=1)
    
    # í•™ìŠµ ì‹œì‘
    print("\nğŸš€ í•™ìŠµ ì‹œì‘!")
    print("ğŸ“Š TensorBoard ëª¨ë‹ˆí„°ë§:")
    print(f"   tensorboard --logdir logs/realtime/{args.game}")
    print("\nâ¹ï¸  ESC í‚¤ë¥¼ ëˆŒëŸ¬ ì•ˆì „í•˜ê²Œ ì¤‘ì§€")
    print("=" * 60)
    
    try:
        model.learn(
            total_timesteps=args.timesteps,
            callback=[checkpoint_callback, training_callback],
            progress_bar=True
        )
    except KeyboardInterrupt:
        print("\nâ¹ï¸  í•™ìŠµ ì¤‘ë‹¨ë¨ (Ctrl+C)")
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
    finally:
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_dir = Path(f"models/realtime/{args.game}")
        final_model_dir.mkdir(parents=True, exist_ok=True)
        final_model_path = final_model_dir / f"{args.game}_ppo_realtime_final.zip"
        
        model.save(str(final_model_path))
        env.close()
        
        print("\n" + "=" * 60)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥: {final_model_path}")
        print("=" * 60)
        
        if training_callback.episode_rewards:
            print(f"\nğŸ“ˆ í•™ìŠµ í†µê³„:")
            print(f"   ì´ ì—í”¼ì†Œë“œ: {len(training_callback.episode_rewards)}")
            print(f"   í‰ê·  ë³´ìƒ: {sum(training_callback.episode_rewards) / len(training_callback.episode_rewards):.2f}")
            print(f"   ìµœê³  ë³´ìƒ: {max(training_callback.episode_rewards):.2f}")
        
        print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ë°©ë²•:")
        print(f"   py tools/test_agent_gui.py")
        print(f"   â†’ ëª¨ë¸ ì„ íƒ: {final_model_path}")


if __name__ == "__main__":
    main()
