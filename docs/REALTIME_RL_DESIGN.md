"""
실시간 RL 시스템 설계 문서

## 시스템 구성

### 1. 게임 상태 감지 (reward_detector.py)
- 경험치 바 감지 → 몬스터 처치 보상
- HP 바 감지 → 피격 패널티
- 화면 변화 감지 → 움직임/전투 보상

### 2. 실시간 환경 (rl_env_realtime.py)
- 게임과 직접 상호작용
- 행동 → 키보드 입력 → 게임 반응 → 보상
- 에피소드: 1000 스텝 또는 HP 0

### 3. 학습 파이프라인
```
게임 실행 → 환경 초기화 → 에이전트 학습
    ↓           ↓              ↓
  화면 캡처   상태 관측      행동 선택
    ↓           ↓              ↓
  보상 계산   경험 저장      정책 업데이트
```

## 핵심 개선 사항

### 기존 방식 (오프라인 RL)
- 녹화된 영상만 보고 학습
- 행동과 결과 연결 없음
- 몬스터 추적 불가능

### 실시간 RL
- 실제 게임과 상호작용
- 행동 → 보상 인과관계 명확
- 몬스터를 잡으면 경험치 증가 감지 → 큰 보상

## 보상 함수 설계

### 즉각 보상 (Step Reward)
- 공격 중 화면 변화: +0.3 (타격 이펙트)
- 텔포 후 이동: +0.2
- 정지 상태: -0.05

### 결과 보상 (Outcome Reward)
- 경험치 획득: +1.0 (몬스터 처치!)
- HP 감소: -0.5 (피격)
- HP 0: -10.0 (사망, 에피소드 종료)

### 행동 보상 (Action Reward)
- 공격/텔포: +0.1
- 이동: +0.05
- Idle: -0.1
- 버프 (쿨타임 중): 무시

## 학습 전략

### Phase 1: 탐험 (Exploration)
- Epsilon = 0.3 (30% 랜덤 행동)
- 다양한 행동 시도
- 보상 신호 학습

### Phase 2: 활용 (Exploitation)
- Epsilon = 0.1 (10% 랜덤 행동)
- 효율적인 패턴 강화
- V→A 사냥 패턴 확립

### Phase 3: 최적화 (Optimization)
- Epsilon = 0.05
- 미세 조정
- 안정적인 사냥

## 안전 장치

### 1. 에피소드 제한
- 최대 1000 스텝 (약 2분)
- 사망 시 즉시 종료
- 자동 리셋

### 2. 학습 중단
- ESC 키: 안전 중지
- HP 낮으면 회피 행동 강제
- 포션 자동 사용 (옵션)

### 3. 모니터링
- 실시간 보상 로그
- 에피소드별 통계
- TensorBoard 시각화

## 구현 순서

1. ✅ 경험치 바 ROI 설정 도구
2. ✅ 실시간 환경 구현
3. ⏳ 학습 스크립트 작성
4. ⏳ 테스트 및 디버깅
5. ⏳ 하이퍼파라미터 튜닝

## 예상 학습 시간

- 초기 탐험: 1-2시간 (10k 스텝)
- 패턴 학습: 3-4시간 (30k 스텝)
- 최적화: 2-3시간 (20k 스텝)
- **총 6-9시간** (게임 켜둔 채로)

## 주의사항

⚠️ 게임을 계속 켜두고 학습해야 함
⚠️ 마우스/키보드를 건드리면 안 됨
⚠️ 학습 중 사망 시 자동 부활 필요
⚠️ 게임 창이 최상단에 있어야 함
"""
